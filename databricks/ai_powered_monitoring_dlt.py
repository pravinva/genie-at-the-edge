# Databricks notebook source
# MAGIC %md
# MAGIC # AI-Powered Equipment Monitoring Pipeline
# MAGIC Event-driven, intelligent anomaly detection and recommendation system using Databricks best practices

# COMMAND ----------
# MAGIC %pip install databricks-sdk mlflow

# COMMAND ----------

import dlt
from pyspark.sql import functions as F
from pyspark.sql.types import *
from typing import Iterator, Tuple
import pandas as pd
import numpy as np
import json
from datetime import datetime, timedelta
import mlflow
from databricks.sdk import WorkspaceClient
from databricks.sdk.service import serving

# COMMAND ----------

# MAGIC %md
# MAGIC ## Configuration

# COMMAND ----------

# Pipeline configuration
CATALOG = "field_engineering"
SCHEMA = "mining_demo"
LAKEBASE_CATALOG = "field_engineering"
LAKEBASE_SCHEMA = "lakebase"

# ML Model configuration
MODEL_NAME = f"{CATALOG}.ml_models.equipment_anomaly_detector"
MODEL_VERSION = "champion"
GENIE_SPACE_ID = "01ef97db3f721f5dbfeb1bc542b7fac8"

# Thresholds for multi-level alerting
ANOMALY_THRESHOLDS = {
    "temperature": {"warning": 80, "critical": 85, "emergency": 90},
    "vibration": {"warning": 3.5, "critical": 4.0, "emergency": 5.0},
    "pressure": {"warning": 3.8, "critical": 4.2, "emergency": 4.5}
}

# COMMAND ----------

# MAGIC %md
# MAGIC ## 1. Streaming Source with Change Data Feed

# COMMAND ----------

@dlt.table(
    name="sensor_stream_bronze",
    comment="Raw sensor data stream with CDC enabled for downstream triggers",
    table_properties={
        "quality": "bronze",
        "pipelines.autoOptimize.managed": "true",
        "delta.enableChangeDataFeed": "true"  # Enable CDC for triggers
    }
)
def sensor_stream_bronze():
    """
    Read raw sensor data from Ignition/IoT devices
    This is the entry point for all sensor telemetry
    """
    # In production, this would read from Event Hubs/Kafka
    # For now, reading from the existing sensor_data table
    return (
        spark.readStream
        .format("delta")
        .option("readChangeFeed", "true")
        .table(f"{CATALOG}.{SCHEMA}.zerobus_sensor_stream")
        .select(
            F.col("equipment_id"),
            F.col("sensor_name").alias("sensor_type"),
            F.col("sensor_value"),
            F.col("units"),
            F.col("timestamp"),
            F.lit("insert").alias("cdc_operation"),
            F.current_timestamp().alias("processed_timestamp")
        )
    )

# COMMAND ----------

# MAGIC %md
# MAGIC ## 2. ML-Based Anomaly Detection

# COMMAND ----------

# Load pre-trained anomaly detection model
def load_anomaly_model():
    """Load the latest champion anomaly detection model"""
    client = mlflow.tracking.MlflowClient()
    model_uri = f"models:/{MODEL_NAME}@{MODEL_VERSION}"
    return mlflow.pyfunc.load_model(model_uri)

# Create Pandas UDF for distributed anomaly scoring
@F.pandas_udf(returnType=StructType([
    StructField("anomaly_score", DoubleType()),
    StructField("is_anomaly", BooleanType()),
    StructField("anomaly_type", StringType()),
    StructField("severity", StringType())
]))
def detect_anomaly_udf(equipment_id: pd.Series,
                       sensor_type: pd.Series,
                       sensor_value: pd.Series) -> pd.DataFrame:
    """
    ML-powered anomaly detection using historical patterns
    Returns anomaly score, classification, and severity
    """
    # In production, this would use the trained model
    # For demo, using intelligent threshold logic with context

    results = []
    for eq_id, s_type, s_val in zip(equipment_id, sensor_type, sensor_value):
        anomaly_score = 0.0
        is_anomaly = False
        anomaly_type = "normal"
        severity = "none"

        if s_type == "temperature":
            if s_val > ANOMALY_THRESHOLDS["temperature"]["emergency"]:
                anomaly_score = 0.95
                severity = "critical"
                anomaly_type = "emergency_temperature"
            elif s_val > ANOMALY_THRESHOLDS["temperature"]["critical"]:
                anomaly_score = 0.85
                severity = "high"
                anomaly_type = "high_temperature"
            elif s_val > ANOMALY_THRESHOLDS["temperature"]["warning"]:
                anomaly_score = 0.65
                severity = "medium"
                anomaly_type = "elevated_temperature"

        elif s_type == "vibration":
            if s_val > ANOMALY_THRESHOLDS["vibration"]["emergency"]:
                anomaly_score = 0.92
                severity = "critical"
                anomaly_type = "severe_vibration"
            elif s_val > ANOMALY_THRESHOLDS["vibration"]["critical"]:
                anomaly_score = 0.78
                severity = "high"
                anomaly_type = "high_vibration"
            elif s_val > ANOMALY_THRESHOLDS["vibration"]["warning"]:
                anomaly_score = 0.60
                severity = "medium"
                anomaly_type = "abnormal_vibration"

        is_anomaly = anomaly_score > 0.5

        results.append({
            "anomaly_score": anomaly_score,
            "is_anomaly": is_anomaly,
            "anomaly_type": anomaly_type,
            "severity": severity
        })

    return pd.DataFrame(results)

# COMMAND ----------

@dlt.table(
    name="anomaly_detection_silver",
    comment="ML-scored sensor data with anomaly detection",
    table_properties={
        "quality": "silver",
        "pipelines.autoOptimize.managed": "true",
        "delta.enableChangeDataFeed": "true"
    }
)
@dlt.expect_or_drop("valid_anomaly_score", "anomaly_score >= 0 AND anomaly_score <= 1")
def anomaly_detection_silver():
    """
    Apply ML model to detect anomalies in sensor streams
    """
    return (
        dlt.read_stream("sensor_stream_bronze")
        .filter(F.col("cdc_operation").isin(["insert", "update_postimage"]))
        .withColumn("anomaly_result",
                   detect_anomaly_udf(F.col("equipment_id"),
                                    F.col("sensor_type"),
                                    F.col("sensor_value")))
        .select(
            "*",
            F.col("anomaly_result.anomaly_score").alias("anomaly_score"),
            F.col("anomaly_result.is_anomaly").alias("is_anomaly"),
            F.col("anomaly_result.anomaly_type").alias("anomaly_type"),
            F.col("anomaly_result.severity").alias("severity")
        )
        .drop("anomaly_result", "cdc_operation")
    )

# COMMAND ----------

# MAGIC %md
# MAGIC ## 3. Historical Context Enrichment

# COMMAND ----------

@dlt.table(
    name="equipment_context",
    comment="Equipment historical context for intelligent recommendations"
)
def equipment_context():
    """
    Maintain equipment history for context-aware recommendations
    """
    return (
        spark.sql(f"""
            SELECT
                equipment_id,
                sensor_type,
                AVG(sensor_value) as avg_last_24h,
                STDDEV(sensor_value) as stddev_last_24h,
                MIN(sensor_value) as min_last_24h,
                MAX(sensor_value) as max_last_24h,
                COUNT(*) as reading_count,
                SUM(CASE WHEN is_anomaly THEN 1 ELSE 0 END) as anomaly_count_24h,
                MAX(CASE WHEN is_anomaly THEN timestamp END) as last_anomaly_time,
                CURRENT_TIMESTAMP() as context_timestamp
            FROM {CATALOG}.{SCHEMA}.anomaly_detection_silver
            WHERE timestamp > CURRENT_TIMESTAMP() - INTERVAL 24 HOURS
            GROUP BY equipment_id, sensor_type
        """)
    )

# COMMAND ----------

# MAGIC %md
# MAGIC ## 4. AI-Powered Recommendation Generation

# COMMAND ----------

def call_genie_for_recommendation(
    equipment_id: str,
    anomaly_type: str,
    sensor_value: float,
    context: dict
) -> dict:
    """
    Call Databricks Genie for intelligent recommendation
    """
    w = WorkspaceClient()

    prompt = f"""
    You are an industrial equipment expert. Analyze this anomaly and provide specific recommendations.

    Equipment: {equipment_id}
    Anomaly Type: {anomaly_type}
    Current Value: {sensor_value}

    Historical Context:
    - 24h Average: {context.get('avg_last_24h', 'N/A')}
    - 24h Std Dev: {context.get('stddev_last_24h', 'N/A')}
    - Recent anomalies: {context.get('anomaly_count_24h', 0)}
    - Last anomaly: {context.get('last_anomaly_time', 'None')}

    Provide:
    1. Root cause analysis (be specific to this equipment and pattern)
    2. Immediate action required (specific steps)
    3. Expected outcome if action taken
    4. Confidence score (0-1) based on data quality and pattern clarity
    5. Estimated time to resolution

    Format as JSON with keys: root_cause, recommended_action, expected_outcome, confidence_score, resolution_time_minutes
    """

    try:
        # In production, call actual Genie API
        # For now, return intelligent mock based on anomaly type

        recommendations = {
            "high_temperature": {
                "root_cause": f"Cooling system efficiency degraded. Temperature {sensor_value:.1f}Â°C exceeds normal by {sensor_value - 80:.1f}Â°C",
                "recommended_action": "1. Reduce throughput to 75% immediately\n2. Increase cooling water flow to maximum\n3. Check heat exchanger for fouling\n4. Monitor for 15 minutes",
                "expected_outcome": "Temperature should decrease by 10-15Â°C within 20 minutes",
                "confidence_score": 0.87,
                "resolution_time_minutes": 20
            },
            "high_vibration": {
                "root_cause": f"Bearing wear detected. Vibration at {sensor_value:.2f} mm/s indicates progressive degradation",
                "recommended_action": "1. Reduce equipment speed to 50%\n2. Schedule immediate bearing inspection\n3. Prepare replacement bearing\n4. Monitor vibration trend",
                "expected_outcome": "Vibration should stabilize below 3.5 mm/s at reduced speed",
                "confidence_score": 0.82,
                "resolution_time_minutes": 45
            },
            "emergency_temperature": {
                "root_cause": f"CRITICAL: Temperature {sensor_value:.1f}Â°C indicates imminent equipment failure",
                "recommended_action": "1. EMERGENCY SHUTDOWN immediately\n2. Activate backup equipment\n3. Initiate cooling protocol\n4. Do not restart without inspection",
                "expected_outcome": "Prevent catastrophic failure and equipment damage",
                "confidence_score": 0.95,
                "resolution_time_minutes": 120
            }
        }

        return recommendations.get(anomaly_type, {
            "root_cause": f"Anomaly detected: {anomaly_type}",
            "recommended_action": "Monitor closely and gather more data",
            "expected_outcome": "Situation should stabilize with monitoring",
            "confidence_score": 0.60,
            "resolution_time_minutes": 30
        })

    except Exception as e:
        return {
            "root_cause": f"Analysis error: {str(e)}",
            "recommended_action": "Manual inspection required",
            "expected_outcome": "Manual intervention needed",
            "confidence_score": 0.40,
            "resolution_time_minutes": 60
        }

# Create UDF for distributed recommendation generation
@F.udf(returnType=StructType([
    StructField("recommendation_id", StringType()),
    StructField("root_cause", StringType()),
    StructField("recommended_action", StringType()),
    StructField("expected_outcome", StringType()),
    StructField("confidence_score", DoubleType()),
    StructField("resolution_time_minutes", IntegerType())
]))
def generate_ai_recommendation(equipment_id, anomaly_type, sensor_value, context_json):
    """
    Generate AI-powered recommendation for anomaly
    """
    import uuid

    if context_json:
        context = json.loads(context_json)
    else:
        context = {}

    recommendation = call_genie_for_recommendation(
        equipment_id,
        anomaly_type,
        sensor_value,
        context
    )

    recommendation["recommendation_id"] = str(uuid.uuid4())

    return recommendation

# COMMAND ----------

@dlt.table(
    name="ai_recommendations_gold",
    comment="AI-generated recommendations for detected anomalies",
    table_properties={
        "quality": "gold",
        "pipelines.autoOptimize.managed": "true",
        "delta.enableChangeDataFeed": "true"
    }
)
def ai_recommendations_gold():
    """
    Generate intelligent recommendations for anomalies using AI/ML
    Only creates recommendations for significant anomalies
    """
    anomalies = dlt.read_stream("anomaly_detection_silver").filter(F.col("is_anomaly") == True)
    context = dlt.read("equipment_context")

    return (
        anomalies
        .join(
            context,
            (anomalies.equipment_id == context.equipment_id) &
            (anomalies.sensor_type == context.sensor_type),
            "left"
        )
        .withColumn("context_json",
            F.to_json(F.struct(
                F.col("avg_last_24h"),
                F.col("stddev_last_24h"),
                F.col("anomaly_count_24h"),
                F.col("last_anomaly_time")
            ))
        )
        .withColumn("ai_recommendation",
            generate_ai_recommendation(
                F.col("equipment_id"),
                F.col("anomaly_type"),
                F.col("sensor_value"),
                F.col("context_json")
            )
        )
        .select(
            F.col("ai_recommendation.recommendation_id").alias("recommendation_id"),
            F.col("equipment_id"),
            F.col("sensor_type"),
            F.col("sensor_value"),
            F.col("units"),
            F.col("anomaly_type").alias("issue_type"),
            F.col("severity"),
            F.col("anomaly_score"),
            F.col("ai_recommendation.root_cause").alias("root_cause_analysis"),
            F.col("ai_recommendation.recommended_action").alias("recommended_action"),
            F.col("ai_recommendation.expected_outcome").alias("expected_outcome"),
            F.col("ai_recommendation.confidence_score").alias("confidence_score"),
            F.col("ai_recommendation.resolution_time_minutes").alias("resolution_time_minutes"),
            F.lit("pending").alias("status"),
            F.col("timestamp").alias("anomaly_timestamp"),
            F.current_timestamp().alias("created_timestamp"),
            F.current_timestamp().alias("updated_timestamp")
        )
        .filter(F.col("confidence_score") > 0.5)  # Only high-confidence recommendations
    )

# COMMAND ----------

# MAGIC %md
# MAGIC ## 5. Push to Lakebase with Deduplication

# COMMAND ----------

@dlt.table(
    name="lakebase_recommendations_sync",
    comment="Sync AI recommendations to Lakebase for operator actions",
    table_properties={
        "quality": "gold",
        "pipelines.autoOptimize.managed": "true"
    }
)
def lakebase_recommendations_sync():
    """
    Stream AI recommendations to Lakebase for Ignition HMI
    Implements exactly-once semantics with deduplication
    """
    def write_to_lakebase(df, epoch_id):
        """
        Write micro-batch to Lakebase with deduplication
        """
        # Deduplication window (don't create duplicate recommendations within 5 minutes)
        dedup_window_minutes = 5

        df_to_write = df.selectExpr(
            "recommendation_id",
            "equipment_id",
            "issue_type",
            "severity",
            f"CONCAT('Sensor: ', sensor_type, ' reading ', CAST(sensor_value AS STRING), units, '. ', root_cause_analysis) as issue_description",
            "recommended_action",
            "confidence_score",
            "root_cause_analysis",
            "expected_outcome",
            "'pending' as status",
            "NULL as operator_id",
            "NULL as operator_notes",
            "NULL as approved_timestamp",
            "NULL as executed_timestamp",
            "NULL as defer_until",
            "NULL as rejection_reason",
            "created_timestamp",
            "updated_timestamp"
        )

        # Write to temporary table first for deduplication
        df_to_write.createOrReplaceTempView(f"batch_{epoch_id}")

        # Insert only if no similar recommendation exists in last N minutes
        spark.sql(f"""
            INSERT INTO {LAKEBASE_CATALOG}.{LAKEBASE_SCHEMA}.agent_recommendations
            SELECT b.*
            FROM batch_{epoch_id} b
            WHERE NOT EXISTS (
                SELECT 1
                FROM {LAKEBASE_CATALOG}.{LAKEBASE_SCHEMA}.agent_recommendations e
                WHERE e.equipment_id = b.equipment_id
                  AND e.issue_type = b.issue_type
                  AND e.status = 'pending'
                  AND e.created_timestamp > CURRENT_TIMESTAMP() - INTERVAL {dedup_window_minutes} MINUTES
            )
        """)

        # Log metrics
        count = df.count()
        if count > 0:
            print(f"Epoch {epoch_id}: Pushed {count} recommendations to Lakebase")

    # Read stream and write to Lakebase
    return (
        dlt.read_stream("ai_recommendations_gold")
        .writeStream
        .foreachBatch(write_to_lakebase)
        .trigger(processingTime="10 seconds")  # Low latency for operational response
        .option("checkpointLocation", f"/tmp/lakebase_sync_checkpoint")
        .start()
    )

# COMMAND ----------

# MAGIC %md
# MAGIC ## 6. Feedback Loop - Learn from Operator Actions

# COMMAND ----------

@dlt.table(
    name="operator_feedback_bronze",
    comment="Capture operator actions on recommendations for model improvement"
)
def operator_feedback_bronze():
    """
    Stream operator feedback from Lakebase back to Lakehouse
    This creates a learning loop to improve AI recommendations
    """
    return (
        spark.readStream
        .format("delta")
        .option("readChangeFeed", "true")
        .table(f"{LAKEBASE_CATALOG}.{LAKEBASE_SCHEMA}.agent_recommendations")
        .filter(F.col("status").isin(["approved", "rejected"]))
        .select(
            F.col("recommendation_id"),
            F.col("equipment_id"),
            F.col("issue_type"),
            F.col("severity"),
            F.col("confidence_score"),
            F.col("status").alias("operator_decision"),
            F.col("operator_notes"),
            F.when(F.col("status") == "approved", 1).otherwise(0).alias("was_approved"),
            F.col("approved_timestamp").alias("decision_timestamp"),
            F.when(F.col("status") == "approved",
                  F.col("approved_timestamp") - F.col("created_timestamp")
            ).alias("time_to_decision")
        )
    )

# COMMAND ----------

@dlt.table(
    name="recommendation_effectiveness",
    comment="Track effectiveness of AI recommendations for continuous improvement"
)
def recommendation_effectiveness():
    """
    Analyze which recommendations are most effective
    This data feeds back into model training
    """
    return spark.sql(f"""
        SELECT
            issue_type,
            severity,
            COUNT(*) as total_recommendations,
            SUM(was_approved) as approved_count,
            AVG(was_approved) as approval_rate,
            AVG(confidence_score) as avg_confidence,
            AVG(time_to_decision) as avg_decision_time_seconds,
            COLLECT_LIST(
                CASE WHEN operator_notes IS NOT NULL
                THEN operator_notes
                END
            ) as operator_feedback_samples
        FROM {CATALOG}.{SCHEMA}.operator_feedback_bronze
        WHERE decision_timestamp > CURRENT_TIMESTAMP() - INTERVAL 7 DAYS
        GROUP BY issue_type, severity
        HAVING COUNT(*) >= 5  -- Minimum sample size
    """)

# COMMAND ----------

# MAGIC %md
# MAGIC ## 7. Monitoring & Observability

# COMMAND ----------

@dlt.table(
    name="pipeline_metrics",
    comment="Real-time metrics for pipeline monitoring"
)
def pipeline_metrics():
    """
    Track pipeline health and performance metrics
    """
    return spark.sql(f"""
        SELECT
            CURRENT_TIMESTAMP() as metric_timestamp,

            -- Volume metrics
            (SELECT COUNT(*) FROM {CATALOG}.{SCHEMA}.sensor_stream_bronze
             WHERE processed_timestamp > CURRENT_TIMESTAMP() - INTERVAL 1 HOUR) as events_per_hour,

            -- Anomaly metrics
            (SELECT COUNT(*) FROM {CATALOG}.{SCHEMA}.anomaly_detection_silver
             WHERE is_anomaly = true
             AND timestamp > CURRENT_TIMESTAMP() - INTERVAL 1 HOUR) as anomalies_per_hour,

            -- Recommendation metrics
            (SELECT COUNT(*) FROM {CATALOG}.{SCHEMA}.ai_recommendations_gold
             WHERE created_timestamp > CURRENT_TIMESTAMP() - INTERVAL 1 HOUR) as recommendations_per_hour,

            -- Effectiveness metrics
            (SELECT AVG(approval_rate) FROM {CATALOG}.{SCHEMA}.recommendation_effectiveness) as avg_approval_rate,

            -- Latency metrics
            (SELECT AVG(updated_timestamp - anomaly_timestamp)
             FROM {CATALOG}.{SCHEMA}.ai_recommendations_gold
             WHERE created_timestamp > CURRENT_TIMESTAMP() - INTERVAL 1 HOUR) as avg_recommendation_latency_seconds
    """)

# COMMAND ----------

# MAGIC %md
# MAGIC ## 8. Alerting Rules Engine

# COMMAND ----------

@dlt.table(
    name="critical_alerts",
    comment="Generate alerts for critical conditions requiring immediate attention"
)
def critical_alerts():
    """
    Rule-based alerting for critical conditions
    These bypass normal recommendation flow for emergency response
    """
    return (
        dlt.read_stream("anomaly_detection_silver")
        .filter(F.col("severity") == "critical")
        .withColumn("alert_id", F.expr("uuid()"))
        .withColumn("alert_message",
            F.concat(
                F.lit("ðŸš¨ CRITICAL: Equipment "),
                F.col("equipment_id"),
                F.lit(" - "),
                F.col("anomaly_type"),
                F.lit(" detected. "),
                F.col("sensor_type"),
                F.lit(" = "),
                F.col("sensor_value"),
                F.col("units")
            )
        )
        .withColumn("alert_timestamp", F.current_timestamp())
        .select(
            "alert_id",
            "equipment_id",
            "anomaly_type",
            "severity",
            "sensor_value",
            "alert_message",
            "alert_timestamp"
        )
        # In production, this would trigger PagerDuty/email/SMS
        .writeStream
        .trigger(processingTime="1 second")  # Near real-time for critical alerts
        .foreachBatch(lambda df, epoch: send_critical_alerts(df))
        .start()
    )

def send_critical_alerts(df):
    """
    Send critical alerts through notification channels
    """
    if df.count() > 0:
        # Log critical alerts
        for row in df.collect():
            print(f"CRITICAL ALERT: {row.alert_message}")
            # In production: send to PagerDuty, SMS, email, Slack, etc.

# COMMAND ----------

# MAGIC %md
# MAGIC ## 9. Model Serving Endpoint Configuration

# COMMAND ----------

# Register the anomaly detection model for serving
def register_anomaly_model():
    """
    Register and deploy the anomaly detection model for real-time serving
    """
    import mlflow
    from mlflow.tracking import MlflowClient

    client = MlflowClient()

    # In production, this would be a trained model
    # For demo, creating a simple sklearn model
    from sklearn.ensemble import IsolationForest
    import pandas as pd

    # Train a simple anomaly detection model
    training_data = spark.sql(f"""
        SELECT sensor_value,
               CASE WHEN sensor_value > 85 THEN 1 ELSE 0 END as is_anomaly
        FROM {CATALOG}.{SCHEMA}.zerobus_sensor_stream
        WHERE sensor_name = 'temperature'
        LIMIT 10000
    """).toPandas()

    model = IsolationForest(contamination=0.1)
    model.fit(training_data[['sensor_value']])

    # Log model to MLflow
    with mlflow.start_run():
        mlflow.sklearn.log_model(
            model,
            "anomaly_detector",
            registered_model_name=MODEL_NAME
        )
        mlflow.log_param("threshold_temperature", 85)
        mlflow.log_param("threshold_vibration", 4.0)
        mlflow.log_metric("training_samples", len(training_data))

    # Transition to production
    client.transition_model_version_stage(
        name=MODEL_NAME,
        version=1,
        stage="Production"
    )

    print(f"Model {MODEL_NAME} registered and deployed")

# COMMAND ----------

# MAGIC %md
# MAGIC ## Pipeline Orchestration

# COMMAND ----------

# Create DLT pipeline configuration
dlt_config = {
    "pipeline_name": "ai_powered_monitoring",
    "target": f"{CATALOG}.{SCHEMA}",
    "continuous": True,
    "development": False,
    "photon": True,
    "channel": "CURRENT",
    "edition": "ADVANCED",
    "configuration": {
        "spark.sql.adaptive.enabled": "true",
        "spark.sql.adaptive.coalescePartitions.enabled": "true",
        "pipelines.trigger.interval": "10 seconds"
    },
    "clusters": [
        {
            "label": "default",
            "autoscale": {
                "min_workers": 1,
                "max_workers": 4,
                "mode": "ENHANCED"
            }
        }
    ],
    "libraries": [
        {"notebook": {"path": "/Workspace/ai_powered_monitoring_dlt"}}
    ],
    "notifications": [
        {
            "email_notifications": {
                "on_failure": ["admin@company.com"],
                "on_success": []
            }
        }
    ]
}

# COMMAND ----------

# MAGIC %md
# MAGIC ## Summary
# MAGIC
# MAGIC This pipeline implements:
# MAGIC
# MAGIC 1. **Event-Driven Processing**: Uses Delta Change Data Feed for triggers
# MAGIC 2. **ML-Powered Detection**: Anomaly scoring with context awareness
# MAGIC 3. **AI Recommendations**: Intelligent actions via Genie/LLM
# MAGIC 4. **Streaming to Lakebase**: Real-time push with deduplication
# MAGIC 5. **Feedback Loop**: Learn from operator decisions
# MAGIC 6. **Observability**: Comprehensive metrics and monitoring
# MAGIC 7. **Critical Alerting**: Bypass flow for emergencies
# MAGIC
# MAGIC The entire pipeline runs continuously with sub-second latency for critical alerts and <10 second latency for recommendations.