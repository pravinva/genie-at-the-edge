"""
Mining Operations Delta Live Tables Pipeline - Real-Time Mode
Processes streaming Ignition tag events via Zerobus

Architecture: Bronze (Raw) → Silver (Normalized) → Gold (Analytics)
Target Latency: <1 second end-to-end
Equipment: 5 Haul Trucks, 3 Crushers, 2 Conveyors (107 tags total)

Deployment:
1. Upload this file to Databricks Repos or Workspace
2. Create DLT Pipeline via UI:
   - Workflows → Delta Live Tables → Create Pipeline
   - Name: mining_operations_realtime
   - Target: ignition_genie.mining_demo
   - Mode: Continuous
   - Cluster: Enhanced Autoscaling, Runtime 16.4+, Photon enabled

Author: Generated by Claude Code
Date: 2026-02-15
"""

import dlt
from pyspark.sql.functions import *
from pyspark.sql.types import *

# ============================================
# BRONZE LAYER: Raw from Zerobus
# ============================================

@dlt.table(
    name="tag_events_bronze",
    comment="Raw tag events from Ignition via Zerobus - Real-Time ingestion",
    table_properties={
        "quality": "bronze",
        "pipelines.autoOptimize.zOrderCols": "ingestion_timestamp"
    }
)
def bronze_raw():
    """
    Read from Zerobus-ingested Delta table
    Schema matches ot_event.proto:
    - event_id, event_time, tag_path, tag_provider
    - numeric_value, string_value, boolean_value
    - quality, quality_code, source_system
    - ingestion_timestamp, data_type, alarm_state, alarm_priority
    """
    return (
        spark.readStream
        .table("ignition_genie.mining_ops.tag_events_raw")
        .withColumn("bronze_timestamp", current_timestamp())
    )

# ============================================
# SILVER LAYER: Normalized Equipment Metrics
# ============================================

@dlt.table(
    name="equipment_sensors_normalized",
    comment="Normalized sensor readings with equipment enrichment - Real-Time processing",
    table_properties={
        "quality": "silver",
        "pipelines.trigger.mode": "realtime",
        "pipelines.trigger.checkpoint": "1 minute",
        "delta.autoOptimize.optimizeWrite": "true",
        "delta.autoOptimize.autoCompact": "true"
    }
)
@dlt.expect_or_drop("valid_timestamp", "event_timestamp IS NOT NULL")
@dlt.expect_or_drop("valid_equipment", "equipment_id IS NOT NULL")
@dlt.expect("good_quality", "quality = 'GOOD'")
def silver_normalized():
    """
    Parse tag paths into equipment_id + sensor_name
    Filter to numeric values only
    Enrich with equipment type and location

    Target latency: <500ms from bronze
    """
    bronze = dlt.read_stream("tag_events_bronze")

    return (
        bronze
        # Keep only rows that have an equipment path and numeric value for metrics.
        .filter(col("numeric_value").isNotNull())
        .filter(col("tag_path").contains("/Equipment/"))

        # Parse tag path: [default]Mining/Equipment/HT_001/Speed_KPH
        # Extract: equipment_id (HT_001), sensor_name (Speed_KPH)
        .withColumn("equipment_id",
            regexp_extract(col("tag_path"), r"Equipment/([A-Z_0-9]+)/", 1)
        )
        .withColumn("sensor_name", regexp_extract(col("tag_path"), r"/([^/]+)$", 1))

        # Classify equipment type from ID prefix
        .withColumn("equipment_type",
            when(col("equipment_id").startswith("HT_"), "Haul Truck")
            .when(col("equipment_id").startswith("CR_"), "Crusher")
            .when(col("equipment_id").startswith("CV_"), "Conveyor")
            .otherwise("Unknown")
        )

        # Add location (simulated - would come from equipment_master in production)
        .withColumn("location",
            when(col("equipment_type") == "Haul Truck", "Pit")
            .when(col("equipment_type") == "Crusher", "Primary Processing")
            .when(col("equipment_type") == "Conveyor", "Transfer Station")
            .otherwise("Unknown")
        )

        # Add criticality level
        .withColumn("criticality",
            when(col("sensor_name").contains("Vibration"), "high")
            .when(col("sensor_name").contains("Temp"), "high")
            .when(col("sensor_name").contains("Bearing"), "high")
            .when(col("sensor_name").contains("Motor"), "high")
            .otherwise("medium")
        )

        # Calculate processing latency
        .withColumn("processing_latency_sec",
            unix_timestamp("bronze_timestamp") - unix_timestamp("event_time")
        )

        # Select final schema
        .select(
            col("event_id"),
            col("event_time").alias("event_timestamp"),
            col("equipment_id"),
            col("equipment_type"),
            col("sensor_name"),
            col("numeric_value").alias("sensor_value"),
            col("quality"),
            col("quality_code"),
            col("location"),
            col("criticality"),
            col("source_system"),
            col("ingestion_timestamp"),
            col("processing_latency_sec"),
            col("bronze_timestamp")
        )
    )

# ============================================
# GOLD LAYER: 1-Minute Equipment Performance
# ============================================

@dlt.table(
    name="equipment_performance_1min",
    comment="Equipment performance metrics - 1-minute tumbling windows",
    table_properties={
        "quality": "gold",
        "pipelines.trigger.mode": "realtime",
        "pipelines.trigger.checkpoint": "1 minute",
        "delta.enableChangeDataFeed": "true"
    },
    partition_cols=["date"]
)
@dlt.expect_all({
    "valid_equipment": "equipment_id IS NOT NULL",
    "valid_window": "window_start IS NOT NULL"
})
def gold_equipment_1min():
    """
    1-minute aggregates for all equipment sensors
    Used by Genie for recent performance queries

    Target latency: <300ms from silver
    """
    silver = dlt.read_stream("equipment_sensors_normalized")

    return (
        silver
        # Watermark for handling late data (30 seconds)
        .withWatermark("event_timestamp", "30 seconds")

        # Group by 1-minute tumbling windows
        .groupBy(
            window("event_timestamp", "1 minute").alias("time_window"),
            "equipment_id",
            "equipment_type",
            "sensor_name",
            "location",
            "criticality"
        )

        # Compute aggregates
        .agg(
            avg("sensor_value").alias("avg_value"),
            min("sensor_value").alias("min_value"),
            max("sensor_value").alias("max_value"),
            stddev("sensor_value").alias("stddev_value"),
            count("*").alias("reading_count"),
            max("processing_latency_sec").alias("max_latency_sec")
        )

        # Extract window boundaries and add metadata
        .select(
            col("time_window.start").alias("window_start"),
            col("time_window.end").alias("window_end"),
            to_date(col("time_window.start")).alias("date"),
            "equipment_id",
            "equipment_type",
            "sensor_name",
            "location",
            "criticality",
            "avg_value",
            "min_value",
            "max_value",
            "stddev_value",
            "reading_count",
            "max_latency_sec",
            current_timestamp().alias("processed_at")
        )
    )

# ============================================
# GOLD LAYER: Current Equipment Status
# ============================================

@dlt.table(
    name="equipment_current_status",
    comment="Latest sensor value for each equipment - optimized for 'current status' queries",
    table_properties={
        "quality": "gold",
        "pipelines.trigger.mode": "realtime"
    }
)
def gold_current_status():
    """
    Materialized latest sensor values per equipment
    Enables fast "What is the current speed of HT_001?" queries
    """
    silver = dlt.read_stream("equipment_sensors_normalized")

    # Streaming-safe "latest row per key" using max(struct(timestamp, ...)).
    latest = (
        silver
        .withWatermark("event_timestamp", "30 minutes")
        .groupBy("equipment_id", "equipment_type", "sensor_name", "location", "criticality")
        .agg(
            max(
                struct(
                    col("event_timestamp").alias("last_updated"),
                    col("sensor_value").alias("sensor_value"),
                    col("quality").alias("quality")
                )
            ).alias("latest")
        )
    )

    return latest.select(
        "equipment_id",
        "equipment_type",
        "sensor_name",
        col("latest.sensor_value").alias("sensor_value"),
        col("latest.last_updated").alias("last_updated"),
        "location",
        "criticality",
        col("latest.quality").alias("quality")
    )

# ============================================
# GOLD LAYER: Haul Truck Cycle Status
# ============================================

@dlt.table(
    name="haul_truck_cycles",
    comment="Haul truck cycle tracking - load-haul-dump-return states",
    table_properties={
        "quality": "gold",
        "pipelines.trigger.mode": "realtime"
    }
)
def gold_haul_truck_cycles():
    """
    Track haul truck states and cycle progress
    Enables queries like "Which trucks are currently hauling?"
    """
    bronze = dlt.read_stream("tag_events_bronze")

    # Cycle state is string-based in Ignition; derive directly from bronze events.
    cycle_events = (
        bronze
        .filter(col("tag_path").rlike(r"/Equipment/HT_[0-9]+/Cycle_State$"))
        .withColumn("equipment_id", regexp_extract(col("tag_path"), r"/Equipment/(HT_[0-9]+)/", 1))
        .withColumn("location", lit("Pit"))
        .withColumn("event_timestamp", col("event_time").cast(TimestampType()))
        .withColumn("state_value", col("string_value"))
        .filter(col("state_value").isNotNull())
    )

    latest = (
        cycle_events
        .withWatermark("event_timestamp", "30 minutes")
        .groupBy("equipment_id", "location")
        .agg(
            max(
                struct(
                    col("event_timestamp").alias("state_since"),
                    col("state_value").alias("current_state")
                )
            ).alias("latest")
        )
    )

    return latest.select(
        "equipment_id",
        col("latest.current_state").alias("current_state"),
        col("latest.state_since").alias("state_since"),
        "location",
        current_timestamp().alias("updated_at")
    )

# ============================================
# GOLD LAYER: Anomaly Detection (Simple)
# ============================================

@dlt.table(
    name="sensor_anomalies",
    comment="Statistical anomaly detection - values >2 std deviations from baseline",
    table_properties={
        "quality": "gold",
        "pipelines.trigger.mode": "realtime"
    }
)
def gold_anomalies():
    """
    Simple statistical anomaly detection
    Identifies unusual sensor readings for maintenance alerts

    For production: Replace with MLflow model predictions
    """
    perf = dlt.read_stream("equipment_performance_1min")

    # Streaming-safe threshold-based anomaly logic.
    enriched = (
        perf
        .withColumn(
            "threshold",
            when(lower(col("sensor_name")).contains("vibration"), lit(12.0))
            .when(lower(col("sensor_name")).contains("temp"), lit(95.0))
            .when(lower(col("sensor_name")).contains("bearing"), lit(90.0))
            .when(lower(col("sensor_name")).contains("motor"), lit(88.0))
            .otherwise(lit(1.0e9))
        )
        .withColumn("deviation_score", col("avg_value") / col("threshold"))
        .filter(col("avg_value") > col("threshold"))
        .withColumn(
            "anomaly_type",
            when(lower(col("sensor_name")).contains("vibration"), "excessive_vibration")
            .when(lower(col("sensor_name")).contains("temp"), "temperature_anomaly")
            .when(lower(col("sensor_name")).contains("bearing"), "bearing_issue")
            .when(lower(col("sensor_name")).contains("motor"), "motor_stress")
            .otherwise("sensor_anomaly")
        )
        .withColumn(
            "recommendation",
            when(col("anomaly_type") == "excessive_vibration", "Inspect mechanical components and bearing condition.")
            .when(col("anomaly_type") == "temperature_anomaly", "Check cooling system and airflow immediately.")
            .when(col("anomaly_type") == "bearing_issue", "Schedule bearing inspection and lubrication check.")
            .when(col("anomaly_type") == "motor_stress", "Validate motor load and electrical supply stability.")
            .otherwise("Investigate equipment condition and sensor calibration.")
        )
        .withColumn(
            "severity",
            when(col("deviation_score") > 1.5, "CRITICAL")
            .when(col("deviation_score") > 1.2, "HIGH")
            .otherwise("MEDIUM")
        )
        .withColumn("confidence_score", least(lit(0.99), col("deviation_score") / 2.0))
    )

    return enriched.select(
        "window_start",
        "equipment_id",
        "equipment_type",
        "sensor_name",
        "anomaly_type",
        "severity",
        "avg_value",
        col("threshold").alias("baseline_avg"),
        "deviation_score",
        "confidence_score",
        "recommendation",
        current_timestamp().alias("detected_at")
    )

# ============================================
# MONITORING: Pipeline Quality Metrics
# ============================================

@dlt.table(
    name="pipeline_quality_metrics",
    comment="Track pipeline performance and data quality - 1-minute windows"
)
def quality_metrics():
    """
    Monitor pipeline health and latency
    """
    silver = dlt.read_stream("equipment_sensors_normalized")

    return (
        silver
        .groupBy(window("bronze_timestamp", "1 minute"))
        .agg(
            count("*").alias("total_records"),
            avg("processing_latency_sec").alias("avg_latency_sec"),
            max("processing_latency_sec").alias("max_latency_sec"),
            approx_count_distinct("equipment_id").alias("distinct_equipment"),
            approx_count_distinct("sensor_name").alias("distinct_sensors"),
            sum(when(col("quality") == "GOOD", 1).otherwise(0)).alias("good_quality_count"),
            sum(when(col("quality") != "GOOD", 1).otherwise(0)).alias("bad_quality_count")
        )
        .select(
            col("window.start").alias("minute"),
            "total_records",
            "avg_latency_sec",
            "max_latency_sec",
            "distinct_equipment",
            "distinct_sensors",
            "good_quality_count",
            "bad_quality_count",
            (col("good_quality_count") / (col("total_records") + lit(0.01)) * 100).alias("quality_pct")
        )
    )
