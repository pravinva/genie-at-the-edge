"""
Mining Operations Delta Live Tables Pipeline - Real-Time Mode
Processes streaming Ignition tag events via Zerobus

Architecture: Bronze (Raw) → Silver (Normalized) → Gold (Analytics)
Target Latency: <1 second end-to-end
Equipment: 5 Haul Trucks, 3 Crushers, 2 Conveyors (107 tags total)

Deployment:
1. Upload this file to Databricks Repos or Workspace
2. Create DLT Pipeline via UI:
   - Workflows → Delta Live Tables → Create Pipeline
   - Name: mining_operations_realtime
   - Target: ignition_genie.mining_demo
   - Mode: Continuous
   - Cluster: Enhanced Autoscaling, Runtime 16.4+, Photon enabled

Author: Generated by Claude Code
Date: 2026-02-15
"""

import dlt
from pyspark.sql.functions import *
from pyspark.sql.types import *
from pyspark.sql.window import Window

# ============================================
# BRONZE LAYER: Raw from Zerobus
# ============================================

@dlt.table(
    name="tag_events_bronze",
    comment="Raw tag events from Ignition via Zerobus - Real-Time ingestion",
    table_properties={
        "quality": "bronze",
        "pipelines.autoOptimize.zOrderCols": "ingestion_timestamp"
    }
)
def bronze_raw():
    """
    Read from Zerobus-ingested Delta table
    Schema matches ot_event.proto:
    - event_id, event_time, tag_path, tag_provider
    - numeric_value, string_value, boolean_value
    - quality, quality_code, source_system
    - ingestion_timestamp, data_type, alarm_state, alarm_priority
    """
    return (
        spark.readStream
        .table("ignition_genie.mining_ops.tag_events_raw")
        .withColumn("bronze_timestamp", current_timestamp())
    )

# ============================================
# SILVER LAYER: Normalized Equipment Metrics
# ============================================

@dlt.table(
    name="equipment_sensors_normalized",
    comment="Normalized sensor readings with equipment enrichment - Real-Time processing",
    table_properties={
        "quality": "silver",
        "pipelines.trigger.mode": "realtime",
        "pipelines.trigger.checkpoint": "1 minute",
        "delta.autoOptimize.optimizeWrite": "true",
        "delta.autoOptimize.autoCompact": "true"
    }
)
@dlt.expect_or_drop("valid_timestamp", "event_time IS NOT NULL")
@dlt.expect_or_drop("valid_tag_path", "tag_path IS NOT NULL")
@dlt.expect("good_quality", "quality = 'GOOD'")
def silver_normalized():
    """
    Parse tag paths into equipment_id + sensor_name
    Filter to numeric values only
    Enrich with equipment type and location

    Target latency: <500ms from bronze
    """
    bronze = dlt.read_stream("tag_events_bronze")

    return (
        bronze
        # Filter to numeric sensors only (most critical metrics)
        .filter(col("numeric_value").isNotNull())

        # Parse tag path: [default]Mining/Equipment/HT_001/Speed_KPH
        # Extract: equipment_id (HT_001), sensor_name (Speed_KPH)
        .withColumn("equipment_id",
            regexp_extract(col("tag_path"), r"Equipment/([A-Z_0-9]+)/", 1)
        )
        .withColumn("sensor_name",
            regexp_extract(col("tag_path"), r"/([A-Za-z_]+)$", 1)
        )

        # Classify equipment type from ID prefix
        .withColumn("equipment_type",
            when(col("equipment_id").startswith("HT_"), "Haul Truck")
            .when(col("equipment_id").startswith("CR_"), "Crusher")
            .when(col("equipment_id").startswith("CV_"), "Conveyor")
            .otherwise("Unknown")
        )

        # Add location (simulated - would come from equipment_master in production)
        .withColumn("location",
            when(col("equipment_type") == "Haul Truck", "Pit")
            .when(col("equipment_type") == "Crusher", "Primary Processing")
            .when(col("equipment_type") == "Conveyor", "Transfer Station")
            .otherwise("Unknown")
        )

        # Add criticality level
        .withColumn("criticality",
            when(col("sensor_name").contains("Vibration"), "high")
            .when(col("sensor_name").contains("Temp"), "high")
            .when(col("sensor_name").contains("Bearing"), "high")
            .when(col("sensor_name").contains("Motor"), "high")
            .otherwise("medium")
        )

        # Calculate processing latency
        .withColumn("processing_latency_sec",
            unix_timestamp("bronze_timestamp") - unix_timestamp("event_time")
        )

        # Select final schema
        .select(
            col("event_id"),
            col("event_time").alias("event_timestamp"),
            col("equipment_id"),
            col("equipment_type"),
            col("sensor_name"),
            col("numeric_value").alias("sensor_value"),
            col("quality"),
            col("quality_code"),
            col("location"),
            col("criticality"),
            col("source_system"),
            col("ingestion_timestamp"),
            col("processing_latency_sec"),
            col("bronze_timestamp")
        )
    )

# ============================================
# GOLD LAYER: 1-Minute Equipment Performance
# ============================================

@dlt.table(
    name="equipment_performance_1min",
    comment="Equipment performance metrics - 1-minute tumbling windows",
    table_properties={
        "quality": "gold",
        "pipelines.trigger.mode": "realtime",
        "pipelines.trigger.checkpoint": "1 minute",
        "delta.enableChangeDataFeed": "true"
    },
    partition_cols=["date"]
)
@dlt.expect_all({
    "valid_equipment": "equipment_id IS NOT NULL",
    "valid_window": "window_start IS NOT NULL"
})
def gold_equipment_1min():
    """
    1-minute aggregates for all equipment sensors
    Used by Genie for recent performance queries

    Target latency: <300ms from silver
    """
    silver = dlt.read_stream("equipment_sensors_normalized")

    return (
        silver
        # Watermark for handling late data (30 seconds)
        .withWatermark("event_timestamp", "30 seconds")

        # Group by 1-minute tumbling windows
        .groupBy(
            window("event_timestamp", "1 minute").alias("time_window"),
            "equipment_id",
            "equipment_type",
            "sensor_name",
            "location",
            "criticality"
        )

        # Compute aggregates
        .agg(
            avg("sensor_value").alias("avg_value"),
            min("sensor_value").alias("min_value"),
            max("sensor_value").alias("max_value"),
            stddev("sensor_value").alias("stddev_value"),
            count("*").alias("reading_count"),
            max("processing_latency_sec").alias("max_latency_sec")
        )

        # Extract window boundaries and add metadata
        .select(
            col("time_window.start").alias("window_start"),
            col("time_window.end").alias("window_end"),
            date(col("time_window.start")).alias("date"),
            "equipment_id",
            "equipment_type",
            "sensor_name",
            "location",
            "criticality",
            "avg_value",
            "min_value",
            "max_value",
            "stddev_value",
            "reading_count",
            "max_latency_sec",
            current_timestamp().alias("processed_at")
        )
    )

# ============================================
# GOLD LAYER: Current Equipment Status
# ============================================

@dlt.table(
    name="equipment_current_status",
    comment="Latest sensor value for each equipment - optimized for 'current status' queries",
    table_properties={
        "quality": "gold",
        "pipelines.trigger.mode": "realtime"
    }
)
def gold_current_status():
    """
    Materialized latest sensor values per equipment
    Enables fast "What is the current speed of HT_001?" queries
    """
    silver = dlt.read_stream("equipment_sensors_normalized")

    # Get latest reading per equipment + sensor combination
    window_spec = Window.partitionBy("equipment_id", "sensor_name").orderBy(desc("event_timestamp"))

    return (
        silver
        .withColumn("row_num", row_number().over(window_spec))
        .filter(col("row_num") == 1)
        .select(
            "equipment_id",
            "equipment_type",
            "sensor_name",
            "sensor_value",
            col("event_timestamp").alias("last_updated"),
            "location",
            "criticality",
            "quality"
        )
    )

# ============================================
# GOLD LAYER: Haul Truck Cycle Status
# ============================================

@dlt.table(
    name="haul_truck_cycles",
    comment="Haul truck cycle tracking - load-haul-dump-return states",
    table_properties={
        "quality": "gold",
        "pipelines.trigger.mode": "realtime"
    }
)
def gold_haul_truck_cycles():
    """
    Track haul truck states and cycle progress
    Enables queries like "Which trucks are currently hauling?"
    """
    silver = dlt.read_stream("equipment_sensors_normalized")

    # Filter to haul trucks, get Cycle_State sensor
    truck_states = (
        silver
        .filter(col("equipment_type") == "Haul Truck")
        .filter(col("sensor_name") == "Cycle_State")
    )

    # Get latest state per truck
    window_spec = Window.partitionBy("equipment_id").orderBy(desc("event_timestamp"))

    return (
        truck_states
        .withColumn("row_num", row_number().over(window_spec))
        .filter(col("row_num") == 1)
        .select(
            "equipment_id",
            col("sensor_value").alias("current_state"),
            col("event_timestamp").alias("state_since"),
            "location",
            current_timestamp().alias("updated_at")
        )
    )

# ============================================
# GOLD LAYER: Anomaly Detection (Simple)
# ============================================

@dlt.table(
    name="sensor_anomalies",
    comment="Statistical anomaly detection - values >2 std deviations from baseline",
    table_properties={
        "quality": "gold",
        "pipelines.trigger.mode": "realtime"
    }
)
def gold_anomalies():
    """
    Simple statistical anomaly detection
    Identifies unusual sensor readings for maintenance alerts

    For production: Replace with MLflow model predictions
    """
    perf = dlt.read_stream("equipment_performance_1min")

    # Calculate rolling baseline (last 60 minutes = 60 windows)
    window_60min = Window.partitionBy("equipment_id", "sensor_name").orderBy("window_start").rowsBetween(-60, -1)

    return (
        perf
        # Calculate historical baseline
        .withColumn("baseline_avg", avg("avg_value").over(window_60min))
        .withColumn("baseline_stddev", stddev("avg_value").over(window_60min))

        # Compute deviation score
        .withColumn("deviation_score",
            abs(col("avg_value") - col("baseline_avg")) / (col("baseline_stddev") + lit(0.01))  # Avoid div by zero
        )

        # Filter to anomalies only (>2 std deviations)
        .filter(col("deviation_score") > 2.0)

        # Classify anomaly type
        .withColumn("anomaly_type",
            when(col("sensor_name").contains("Vibration"), "excessive_vibration")
            .when(col("sensor_name").contains("Temp"), "temperature_anomaly")
            .when(col("sensor_name").contains("Bearing"), "bearing_issue")
            .when(col("sensor_name").contains("Motor"), "motor_stress")
            .otherwise("sensor_anomaly")
        )

        # Generate recommendation
        .withColumn("recommendation",
            when(col("anomaly_type") == "excessive_vibration",
                "ALERT: Inspect mechanical components. Check belt alignment and bearing condition.")
            .when(col("anomaly_type") == "temperature_anomaly",
                "ALERT: Check cooling system. Verify fan operation and coolant levels.")
            .when(col("anomaly_type") == "bearing_issue",
                "ALERT: Bearing degradation detected. Schedule maintenance inspection.")
            .when(col("anomaly_type") == "motor_stress",
                "ALERT: Motor operating outside normal range. Check load and electrical supply.")
            .otherwise("ALERT: Unusual sensor reading detected. Investigate equipment condition.")
        )

        # Severity (based on deviation magnitude)
        .withColumn("severity",
            when(col("deviation_score") > 4.0, "CRITICAL")
            .when(col("deviation_score") > 3.0, "HIGH")
            .otherwise("MEDIUM")
        )

        # Confidence score
        .withColumn("confidence_score",
            least(lit(0.99), col("deviation_score") / 5.0)
        )

        .select(
            "window_start",
            "equipment_id",
            "equipment_type",
            "sensor_name",
            "anomaly_type",
            "severity",
            "avg_value",
            "baseline_avg",
            "deviation_score",
            "confidence_score",
            "recommendation",
            current_timestamp().alias("detected_at")
        )
    )

# ============================================
# MONITORING: Pipeline Quality Metrics
# ============================================

@dlt.table(
    name="pipeline_quality_metrics",
    comment="Track pipeline performance and data quality - 1-minute windows"
)
def quality_metrics():
    """
    Monitor pipeline health and latency
    """
    silver = dlt.read_stream("equipment_sensors_normalized")

    return (
        silver
        .groupBy(window("bronze_timestamp", "1 minute"))
        .agg(
            count("*").alias("total_records"),
            avg("processing_latency_sec").alias("avg_latency_sec"),
            max("processing_latency_sec").alias("max_latency_sec"),
            approx_count_distinct("equipment_id").alias("distinct_equipment"),
            approx_count_distinct("sensor_name").alias("distinct_sensors"),
            sum(when(col("quality") == "GOOD", 1).otherwise(0)).alias("good_quality_count"),
            sum(when(col("quality") != "GOOD", 1).otherwise(0)).alias("bad_quality_count")
        )
        .select(
            col("window.start").alias("minute"),
            "total_records",
            "avg_latency_sec",
            "max_latency_sec",
            "distinct_equipment",
            "distinct_sensors",
            "good_quality_count",
            "bad_quality_count",
            (col("good_quality_count") / (col("total_records") + lit(0.01)) * 100).alias("quality_pct")
        )
    )
